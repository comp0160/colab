{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/RxlrAhDoFWBW+IM5NFCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comp0160/colab/blob/main/datavis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualisation and Analysis\n",
        "\n",
        "In this notebook we'll look at some data acquired from the psychophysics experiments in the labs, and possibly also some more generic data.\n",
        "\n",
        "We'll be using Python for this, though most of what we're doing is pretty basic and should transfer reasonably easily to environments such as R and Matlab. The code is provided, so if you don't know Python you should still be able to follow along. In some cases we might suggest ways you could tweak the code for different purposes, but you don't *have* to do that.\n",
        "\n",
        "If in doubt, feel free to ask for help from the tutors.\n",
        "\n"
      ],
      "metadata": {
        "id": "6ySki56_qbVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Colab"
      ],
      "metadata": {
        "id": "bTuXvcaEf4Ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory provides free access to computational resources via a web-based front end known as a **notebook**. There are pros and cons to this approach, but it does have the virtue of providing a consistent software environment for everyone without having to wrestle with installation and configuration woes on dozens of different, subtly-incompatible laptops.\n",
        "\n",
        "The notebook content is broken down into **cells**, of which there are two basic types:\n",
        "* **Text** cells (like this one) contain rich text and possibly other media such as images. You should **read** the content of these cells!\n",
        "* **Code** cells (like the one below) contain program instructions in Python, and sometimes other operating system commands. The latter are typically prefixed by an exclamation mark, like this: `!ls -l`\n",
        "   \n",
        "   Code content can be executed on the remote **virtual machine** by clicking on the **play**  ( ▶︎ ) button that appears in the top left hand corner of the cell when you move your cursor over it. Some code may take a while to run — the play button will change to a **stop** (◾️) button, and an animated progress wheel will show around it. As the code runs, it may produce output, which will appear at the bottom of the cell. You can click the stop button to halt the execution.\n",
        "\n",
        "  (The notebook must be **connected** to a virtual machine in order to run code cells. This should happen automatically the first time you attempt to execute a cell — there will be a delay while this takes place.)\n",
        "\n",
        "Try clicking the play button on the cell below to run it."
      ],
      "metadata": {
        "id": "4djncs15HjDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a very simple example of a code cell\n",
        "# running the cell will execute the statement below and generate output\n",
        "print('hello world!')"
      ],
      "metadata": {
        "id": "cjQ9mzQiRSzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two important caveats to be aware of, one to do with the Jupyter notebook interface and the other to do with Colab:\n",
        "\n",
        "1. The notebook interface allows **out of order execution** of code cells. That is, it is possible to run later cells before earlier ones. This is really **never what you want** and can give rise to all kinds of problems with inconsistent state. Always be sure to run cells in order. If you need to go back and re-run earlier cells (eg, because you want to change something) always follow that up by re-running all the subsequent cells in order too, to make sure everything is consistent.\n",
        "\n",
        "2. The virtual machine environment that Colab runs behind this notebook interface is **resource-limited** and **transient**. If you don't do anything for awhile, the notebook may disconnect from the virtual machine. If you use too much computation then your session may be halted — and what qualifies as \"too much\" is kept intentionally vague. VMs are purged daily: if you stop and come back to it tomorrow the VM will have been deleted and you will need to go back to the beginning and go through the setup steps again. If you generate something that you want to keep, you should download it to your local machine.\n"
      ],
      "metadata": {
        "id": "twrVv_VbbIzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up"
      ],
      "metadata": {
        "id": "yF-Bcw-lw36S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be making use of some very standard python packages — [NumPy](https://numpy.org/doc/stable/user/index.html#user), [SciPy](https://docs.scipy.org/doc/scipy/tutorial/general.html), [Pandas](https://pandas.pydata.org/docs/user_guide/index.html), [Matplotlib](https://matplotlib.org/stable/users/index.html), [Seaborn](https://seaborn.pydata.org/index.html) and [Scikit-Learn](https://scikit-learn.org/stable/user_guide.html). These are all sufficiently common that they're part of the default setup on a Colab virtual machine, so we don't have to mess about installing them. But we do have to tell the Python interpreter to import them:"
      ],
      "metadata": {
        "id": "5P0HXe_9xIh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37LKI6tWpcvM"
      },
      "outputs": [],
      "source": [
        "# ubiquitous python packages for data analysis and visualisation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# this is probably the default, but just in case\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set_style('white')\n",
        "sns.set_style('ticks')\n",
        "\n",
        "# we'll probably do a little light model fitting too\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're also going to make use of a less standard package for fitting psychometric functions, [psignifit](https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/neuronale-informationsverarbeitung/research/software/psignifit/). This one is *not* part of the default setup, and also is not available from the python package repository PyPI, so we need to explicitly install it from GitHub."
      ],
      "metadata": {
        "id": "_ySgdZ8WiFb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/wichmann-lab/python-psignifit/zipball/master"
      ],
      "metadata": {
        "id": "bpc5QQIDiGCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, we'll need to tell the interpreter to import it."
      ],
      "metadata": {
        "id": "1ftZfPkFj0Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psignifit as ps"
      ],
      "metadata": {
        "id": "EvyL2rGYj04D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "KyCHD-12m3sZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As well as the code libraries, we'll also need some data to look at. For simplicity we'll fetch some files Matthew prepared earlier, which are available from the module GitHub site."
      ],
      "metadata": {
        "id": "YaIPLWQIqdN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/comp0160/data.git"
      ],
      "metadata": {
        "id": "9_B78meE3ISm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All being well, you should now have a directory called `data`, with some CSV files in it. We can list the contents like this:"
      ],
      "metadata": {
        "id": "hpZnVHOB3hKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "id": "PScZOjyT3qOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might recognise some of these file names as outputs from the experiments in Labs 1—3.\n",
        "\n",
        "You can browse the files on your virtual machine via the little **folder icon** in the sidebar. You can upload and download files there too, for example if you want to **analyse your own data**."
      ],
      "metadata": {
        "id": "6hlZLqArkhNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started: Pitch"
      ],
      "metadata": {
        "id": "G_ZTRyqynsVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by having a look at a fairly simple data file, from the pitch detection experiment in Lab 3. We can load the CSV like this:"
      ],
      "metadata": {
        "id": "ffEyg5oLn42F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitch = pd.read_csv('data/comp160_lab3_pitch.csv')"
      ],
      "metadata": {
        "id": "HiQMjCMDqO23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable `pitch` now contains a [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) [DataFrame](https://pandas.pydata.org/docs/user_guide/dsintro.html) object, which is something like a spreadsheet table: a two-dimensional data structure, where each column is some named attribute and each row is a data sample.\n",
        "\n",
        "We can look at an object in Jupyter by calling `display` on it. This may just print it out as text, but for some kinds of objects (including DataFrames) it may produce something with more complex formatting:"
      ],
      "metadata": {
        "id": "wPJyoHrOqjqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(pitch)"
      ],
      "metadata": {
        "id": "rpgfaWLsq3R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: `display` is implicitly called on the **return value** from a code cell, which will usually be the last thing in it. So you can achieve the same effect with just:"
      ],
      "metadata": {
        "id": "ur_PoVLGI89n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitch"
      ],
      "metadata": {
        "id": "i433l9WsJP3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will make use of this implicit behaviour frequently below, but calling `display` explicitly can sometimes be clearer. And you may want to display things partway through a cell's code, or display more than one thing."
      ],
      "metadata": {
        "id": "nDLwJjEZJXaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrames have a handy `describe` method which produces some summary statistics on the (numeric) data columns:"
      ],
      "metadata": {
        "id": "OOWBEhhqtrAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitch.describe()"
      ],
      "metadata": {
        "id": "VfsFniF4trvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the `direction` column is omitted here because it doesn't make sense to calculate things like the mean of \"down\" and \"up\".\n",
        "\n",
        "In this data, the thing we're really interested in is the detection threshold, which is recorded in the column `thresh`. Looking at the mean and median (equivalent to the 50th percentile), this looks *reasonably* consistent at a little over 13 kHz and we could probably just stop there.\n",
        "\n",
        "Still, recalling the discussion of errors of **habituation** and **anticipation** back in week 1, we might suspect there's some difference according to whether the pitch sweep was ascending or descending.\n",
        "\n",
        "We can extract **subsets** of data in Pandas DataFrames (and [Series](https://pandas.pydata.org/docs/user_guide/dsintro.html), which are the underlying one-dimensional data structure) by indexing them with **logical conditions**.\n",
        "\n",
        "The syntax for this can be a bit unintuitive at first, so let's step through it.\n",
        "\n",
        "Indexing a DataFrame by **column name** returns the Series contained in just that column:"
      ],
      "metadata": {
        "id": "BgwQ0UzeDbZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitch['ascending']"
      ],
      "metadata": {
        "id": "XapLaLrBNzas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we apply a test to this Series, we get back a new Series whose contents are the results of that test applied to each element of the column:"
      ],
      "metadata": {
        "id": "ZfJCSjryOC3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitch['ascending'] == 1"
      ],
      "metadata": {
        "id": "wLA338GnODsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we use such a Boolean Series to index the original DataFrame, it will pick out only those rows where the Series contains `True`:"
      ],
      "metadata": {
        "id": "hp01zIIROaI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitch[pitch['ascending']==1]"
      ],
      "metadata": {
        "id": "_MavV1n2M23n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is itself a DataFrame, and we can call `describe` on it just as with the original `pitch`:"
      ],
      "metadata": {
        "id": "A8ynnBuQPGL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitch[pitch['ascending']==1].describe()"
      ],
      "metadata": {
        "id": "yqAAP6A_PRE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the same way, we can look at the descending data:"
      ],
      "metadata": {
        "id": "uiKhGzZxPf_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pitch[pitch['ascending']!=1].describe()"
      ],
      "metadata": {
        "id": "lPAkb9SVPt2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Here I've specified `!=1` as the condition just to illustrate the **does not equal** operator, but this could just as easily have been specified as `==0`.)\n",
        "\n",
        "Comparing these two summaries, it definitely looks like there's some difference between the threshold estimated from the ascending sweep — around 13.7 kHz — and that from the downward sweep — about 12.5 kHz. This does suggest some degree of habituation — the subject is more inclined to think they can *still* hear the rising sound and still *can't* hear the descending one.\n",
        "\n",
        "This data set is pretty small, though, with only four samples in each direction. That might not be enough to persuade us that the apparent difference is real, rather than just an accident of measurement noise. One way we can assess this is with a [two sample t-test](https://en.wikipedia.org/wiki/Student%27s_t-test). This, along with a lot of other statistical tests, distributions and models is provided by the [`scipy.stats`]() package:"
      ],
      "metadata": {
        "id": "4-mSW7nmP0Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats.ttest_ind(pitch['thresh'][pitch['ascending']==0], pitch['thresh'][pitch['ascending']==1])"
      ],
      "metadata": {
        "id": "p-k9Yl_dUuZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis testing is a fraught business and $p$ values are the last refuge of the scoundrel. Student $t$ tests are perhaps the single most commonly used significance test, and by the same token probably the most *abused*. As a general rule you should be suspicious of claims that the outcome of such a test *proves* anything — and especially of the implication that there's anything magical about arbitrary significance thresholds such a $p < 0.05$ or $p < 0.01$.\n",
        "\n",
        "In this case we have $p$ marginally above 0.01. Is that significant? Maybe, maybe not. With so few samples it's hard to be sure, but we might have a sense that there's something going on there. If it's something we care about, then we could take the t test result as a cue to do more experiments to probe the matter further. As a rule of thumb: it's fine to use things like t tests to help yourself understand the data. It's when you start making grand claims of significance that you need to be cautious.\n",
        "\n",
        "Another thing we can do to help understand the distribution of the data is to **plot** it. (You may recall being urged in earlier practical sessions to **always plot your data**, and it's not the last time you'll hear that exhortation.)\n",
        "\n",
        "There are many different ways to plot data, and we'll explore a few of them below, but the main workhorse is the [scatter plot](https://en.wikipedia.org/wiki/Scatter_plot), which just plots one thing against another. This will probably be your first port of call for most data.\n",
        "\n",
        "Let's make a scatter plot of the threshold values for all our trials:"
      ],
      "metadata": {
        "id": "3HsgXda5aUMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=pitch, x=pitch.index, y='thresh', style='direction', hue='direction')\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
      ],
      "metadata": {
        "id": "5s8Ok4EBh7GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're using the [Seaborn](https://seaborn.pydata.org/index.html) library here, which provides a slightly nicer interface to the plotting functionality of Matplotlib. One of the things Seaborn supports is specifying how we want to represent particular elements of our dataset in the plot — such as having different markers and colours according to direction. However, controlling things like the position of the legend ultimately falls to Matplotlib — that's what the second line above is for, moving the legend outside the plot so it doesn't obscure any of the data.\n",
        "\n",
        "This is a pretty straightforward plot, and we can already see that the downward sweep results are consistently lower than the upward ones, which is consistent with our observation above. On the other hand, it looks like there's no particular effect of experiment order here, and our horizontal axis is not so interesting. Maybe we'd be better off putting the direction on that axis?"
      ],
      "metadata": {
        "id": "EKG3JINozDXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=pitch, x='direction', y='thresh', style='direction', hue='direction')\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
      ],
      "metadata": {
        "id": "taErRkpJvb0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that `direction` is not a number, it's a **categorical variable**. Seaborn has correctly inferred that we want to group the data accordingly and has mapped the values `up` and `down` to different locations on the x axis.\n",
        "\n",
        "There are other ways we might want to represent this data. For example, a [boxplot](https://en.wikipedia.org/wiki/Box_plot) gives a simplified sketch of the data distribution:"
      ],
      "metadata": {
        "id": "MlboC3CO2FBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=pitch, x='direction', y='thresh')"
      ],
      "metadata": {
        "id": "Qj4cfKl-3kw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a good way to summarise larger datasets, but in this case it's probably overkill, since we only have four samples in each box. Presenting the distribution in this form is arguably a bit dishonest — it makes it seem like the distributions are better characterised than they really are. With this few samples it's more informative to see the actual points rather than summarise them.\n",
        "\n",
        "We've probably got as much out of this little dataset as we're likely to, so let's move on to something a bit bigger."
      ],
      "metadata": {
        "id": "Xvn0xfmV4BG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Data: Stroop Test"
      ],
      "metadata": {
        "id": "o0sFKsJUKOl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pitch detection experiment only had 8 data points and two different experimental conditions, down and up. For the Stroop test we ran more trials, and also had more varied conditions. Let's take a look:"
      ],
      "metadata": {
        "id": "_bmKbQuPKYhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stroop = pd.read_csv('data/comp160_lab2_stroop.csv')\n",
        "display(stroop)\n",
        "stroop.describe()"
      ],
      "metadata": {
        "id": "-rAKE7IfKx4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how the display gets truncated for long DataFrames, to avoid filling the page with stuff you probably don't want to wade through.\n",
        "\n",
        "We can see from the summary table that there's really only one relevant numeric variable in this experiment, the reaction time `rt`. Let's start by taking a look at that.\n",
        "\n",
        "We could use another boxplot to get a rough idea of the distribution of reaction times, but let's try a histogram instead:"
      ],
      "metadata": {
        "id": "mnwC7ktFsXtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(data=stroop, x='rt', kde=True)"
      ],
      "metadata": {
        "id": "k3dbm-IaM3GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The `kde=True` parameter enables [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation), which is what produces the smoothed curve overlaid on the bars.)\n",
        "\n",
        "It's hard to be sure, but it looks like this distribution might have more than one **mode**. Since we know the experiment involved different phases with different conditions, it would make sense for the results to occupy different groups. We might try a scatter plot to see if it makes things any clearer:"
      ],
      "metadata": {
        "id": "nqWZK-6sRS9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=stroop, x=stroop.index, y='rt')"
      ],
      "metadata": {
        "id": "BsiQmzN9Ps_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, we can get some sense that things are varying, but we don't yet have the information to unpick it. So let's bring in some more of that information.\n",
        "\n",
        "Most of the other columns in the data are telling us about the experimental conditions and the subject's response. Unfortunately, looking at the rows above we can see that there are some errors and omissions that we ought to clean up first."
      ],
      "metadata": {
        "id": "8Fh-uu9-QJLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(stroop.iloc[:20,])"
      ],
      "metadata": {
        "id": "cnjemM5xacbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first stage, with neutral words, the `correct_response` field is wrong or missing. We'll have to fill in these values. Fortunately, the information is present elsewhere in the row: the `idx` field tells us what colour was used, so we can set the correct response accordingly. As it happens, the correctly indexed values are actually the first three in this column — this is a serendipitous side effect of the bug that caused this data to be wrong in the first place. (In general you will not be able to rely on such an accident and will need to get the data wherever you can. For a simple case like this that wouldn't be onerous, but data cleaning can sometimes be very difficult, time-consuming and tedious.) We can make use of this to get the right values for this stretch:"
      ],
      "metadata": {
        "id": "XpdbrZLhM3ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_neutrals = stroop['correct_response'][stroop['idx'][:15]].values\n",
        "correct_neutrals"
      ],
      "metadata": {
        "id": "RKRBbJ08ep71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nested indexing above will probably seem a bit confusing, so let's pause to unpick it:\n",
        "\n",
        "* `stroop['correct_response']` gives us the whole `correct_response` column. We only care about the first three elements of this, but we also know that the `idx` column only contains the values 0, 1 and 2, so we can just ignore everything after that.\n",
        "* `stroop['idx'][:15]` gives us the first 15 elements in the `idx` column -- some combination of zeros, ones and twos.\n",
        "* The whole expression `stroop['correct_response'][stroop['idx'][:15]]` is using that set of zeros, ones and twos to index the correct responses.\n",
        "* The final `.values` extracts the Pandas Series contents into a simple array. We do this because we don't want to keep the index values associated with each element.\n",
        "\n",
        "We can now assign these fixed values back into the original data frame. There are a number of ways to do this, but for annoying reasons to do with the underlying Pandas implementation, the more intuitive ones will give a warning (though they'll still usually work).\n",
        "\n",
        "Probably the least terrible \"officially approved\" way looks like this:\n"
      ],
      "metadata": {
        "id": "oDck1S3Ok4-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stroop.loc[stroop.iloc[:15].index, 'correct_response'] = correct_neutrals"
      ],
      "metadata": {
        "id": "wN4Ol0apn2jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's pretty ugly, though. You'd think that a simple `:15` would suffice for that first index, but no. In fact, `:14` would actually work, but that's just unacceptably confusing.\n",
        "\n",
        "Okay, now we have the right right answers, let's add a new column to say whether each row is correct:"
      ],
      "metadata": {
        "id": "jJ_wE8Twq2Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stroop['correct'] = (stroop['response'] == stroop['correct_response'])"
      ],
      "metadata": {
        "id": "74ZzRIfSqmnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's add that info into our scatter plot, along with the congruence and target:"
      ],
      "metadata": {
        "id": "Na0vOFjmxm9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=stroop, x=stroop.index, y='rt', style='correct', hue='congruence', size='target')\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
      ],
      "metadata": {
        "id": "K2dJcIomxxwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It definitely seems like things fall into different groups, but now there's probably a bit too much going on.\n",
        "\n",
        "For starters, after all that effort to get the right answers, there probably aren't enough incorrect responses to worry about anyway:"
      ],
      "metadata": {
        "id": "kE6G5p_021yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(~stroop['correct'])"
      ],
      "metadata": {
        "id": "7lODHTEA3TAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It probably wouldn't do any harm to leave these two in the data, but let's restrict ourselves to right answers."
      ],
      "metadata": {
        "id": "54pvAiN0K7ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stroop_ok = stroop[stroop['correct']]\n",
        "\n",
        "sns.scatterplot(data=stroop_ok, x=stroop_ok.index, y='rt', style='target', hue='congruence')\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
      ],
      "metadata": {
        "id": "0ZBQHd-WK77j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By eye, it seems like the early neutral stimuli tend to get faster responses than the later ones where both the verbal and the visual content are vying for attention. And incongruent stimuli tend to get slower responses than congruent. We can break down these groups in a boxplot to confirm the visual impression:"
      ],
      "metadata": {
        "id": "2kW6xUITMB_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=stroop_ok, x='target', y='rt', hue='congruence')\n",
        "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
      ],
      "metadata": {
        "id": "NlzIgxouMCtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, these apparent differences could still be the result of random variation. We might like to run a few $t$ tests to check whether there's really any difference. However, this is something that needs to done with caution, especially if you're going to make any claims about significance. The problem with [multiple comparisons](https://en.wikipedia.org/wiki/Multiple_comparisons_problem) is that sooner or later *some* of your tests will come up positive just by chance, even when there's no real phenomenon underlying the observations.\n",
        "\n",
        "Multiple comparisons and [spurious correlations](https://www.tylervigen.com/spurious-correlations) can be a big issue when people go on **fishing expeditions**, trawling through huge datasets in search of patterns to exploit. This is something that plagues data mining, especially in grubbier financial contexts where people want to sell you **secret knowledge** that will let you **get rich quick**.\n",
        "\n",
        "Obviously that isn't the case here.\n",
        "\n",
        "There are a variety of strategies for combatting multiple comparisons, of which possibly the simplest and most widely used is the [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction), which basically divides the significance by the number of tests performed. So if you want to perform a significance test for $p < 0.05$ on 5 different data relationships you actually require $p < 0.01$ on each test for it to count. (Equivalently, you can multiply the p-values by the number of tests and keep the original threshold.) The more things you test, the harder it is for any of them to reach the threshold of significance.\n",
        "\n",
        "Here we again don't care about significance, we're just getting a feel for the data. We'll do a few $t$ tests with Bonferroni corrections, but avoid claims of significance anyway."
      ],
      "metadata": {
        "id": "ljBeXhBLN0kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rt_neutrals = stroop_ok[stroop_ok['congruence']=='neutral']['rt'].values\n",
        "rt_colour = stroop_ok[stroop_ok['target']=='colour']['rt'].values\n",
        "rt_congruent = stroop_ok[stroop_ok['congruence']=='congruent']['rt'].values\n",
        "rt_incongruent = stroop_ok[stroop_ok['congruence']=='incongruent']['rt'].values\n",
        "rt_congruent_colour = stroop_ok[(stroop_ok['congruence']=='congruent') & (stroop_ok['target']=='colour')]['rt'].values\n",
        "rt_incongruent_colour = stroop_ok[(stroop_ok['congruence']=='incongruent') & (stroop_ok['target']=='colour')]['rt'].values\n",
        "rt_word = stroop_ok[stroop_ok['target']=='word']['rt'].values\n",
        "rt_congruent_word = stroop_ok[(stroop_ok['congruence']=='congruent') & (stroop_ok['target']=='word')]['rt'].values\n",
        "rt_incongruent_word = stroop_ok[(stroop_ok['congruence']=='incongruent') & (stroop_ok['target']=='word')]['rt'].values\n",
        "\n",
        "n_test = 6\n",
        "print(f'neutral vs colour: {stats.ttest_ind(rt_neutrals, rt_colour)[1] * n_test:.4f}')\n",
        "print(f'neutral vs word: {stats.ttest_ind(rt_neutrals, rt_word)[1] * n_test:.4f}')\n",
        "print(f'colour vs word: {stats.ttest_ind(rt_colour, rt_word)[1] * n_test:.4f}')\n",
        "print(f'congruent vs incongruent: {stats.ttest_ind(rt_congruent, rt_incongruent)[1] * n_test:.4f}')\n",
        "print(f'congruent colour vs incongruent colour: {stats.ttest_ind(rt_congruent_colour, rt_incongruent_colour)[1] * n_test:.4f}')\n",
        "print(f'congruent word vs incongruent word: {stats.ttest_ind(rt_congruent_word, rt_incongruent_word)[1] * n_test:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "Gzd8RRJQVqqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probably those results pretty much accord with what you inferred by eye from the boxplot: some of the distributions were a bit further away than others.\n",
        "\n",
        "Of course, there could always be other causes not captured in the experimental data. Perhaps there were a lot of environmental distractions during the colour trials and things calmed down in time for the word versions?"
      ],
      "metadata": {
        "id": "7AbdWZRBZ8Zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Fitting"
      ],
      "metadata": {
        "id": "3dRidLD1rW4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we've picked out some qualitative differences in the data and estimated some stats. But we haven't tried to figure out a quantitative relationship between the measured variables, or fit a line or curve to the relationship.\n",
        "\n",
        "Let's see if we can do that with some threshold data."
      ],
      "metadata": {
        "id": "xSRhkRaJrd79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stim = pd.read_csv('data/comp160_lab1_const_stim.csv')\n",
        "display(stim)\n",
        "stim.describe()\n",
        "sns.scatterplot(data=stim, x='colour', y='response')\n"
      ],
      "metadata": {
        "id": "c25bkf0QchFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This scatter plot is a bit unclear because results from repeated trials are plotted on top of one another. One way around this is to add a bit of jitter."
      ],
      "metadata": {
        "id": "pJR4XD1veoBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.stripplot(data=stim, x='colour', y='response', orient='h', order=[1, 0])"
      ],
      "metadata": {
        "id": "diwmBJgle6za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we're probably more interested in the **mean** of all the trials at each colour value. We can get this by [grouping](https://pandas.pydata.org/docs/user_guide/groupby.html) the data:"
      ],
      "metadata": {
        "id": "SnCejqeskWxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grpd = stim.groupby(['colour']).mean()\n",
        "display(grpd)"
      ],
      "metadata": {
        "id": "pTfxIfyrhO79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot from this new aggregated table just like the original, but note that the column we grouped on, `colour`, is no longer a column in the new table, it's the table's index."
      ],
      "metadata": {
        "id": "_vMLSb3vk2SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=grpd, x=grpd.index, y='response')"
      ],
      "metadata": {
        "id": "DyVY-x9LkItA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "T_0io4wmu_j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have our data and we want to fit a model to it. The simplest kind of model we're likely to want to fit is a *linear* one: where the output value is just a weighted sum of the inputs (plus a constant intercept, usually). For a scalar input, like `colour` here, and scalar output (like `response`) this just produces a straight line. More complex models might produce planes or hyperplanes, but the basic idea is the same.\n",
        "\n",
        "The standard method for this is known as **linear regression**. Fitting a straight line by linear regression is *extremely* common. Excel will do it for you, for example. And Seaborn will too. Eg, to the original data:"
      ],
      "metadata": {
        "id": "8tr1XTbrxwON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.regplot(data=stim, x='colour', y='response')"
      ],
      "metadata": {
        "id": "l4DTEW15nU6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, equivalently to the grouped averages:"
      ],
      "metadata": {
        "id": "OSABv7pdzmEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.regplot(data=grpd, x=grpd.index, y='response')"
      ],
      "metadata": {
        "id": "Dh9DLOJ1m__k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fun fact: in this case both versions of the dataset produce the same fit. That's not always the case. For example, if the groups were different sizes then the fit would probably be different.\n",
        "\n",
        "Rather than just letting Seaborn plot a line, we can use the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model from [Scikit-Learn](https://scikit-learn.org/stable/user_guide.html) to do the fitting explicitly. One minor wrinkle: Scikit-Learn models can have multiple input features, so they expect to be passed a 2D array of input data. The `reshape(-1,1)` in the call below does that, converting an n-vector into an 1xn matrix."
      ],
      "metadata": {
        "id": "LUJ5-bcKzvnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear = LinearRegression()\n",
        "linear.fit(stim['colour'].values.reshape(-1,1), stim['response'])\n",
        "print(f'slope: {linear.coef_[0]:.4f}, intercept: {linear.intercept_:.4f}')"
      ],
      "metadata": {
        "id": "Th8rAc-YminO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once again, if we fit to the grouped data we get the same result:"
      ],
      "metadata": {
        "id": "CIjWwh9C42Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin_mean = LinearRegression()\n",
        "lin_mean.fit(grpd.index.values.reshape(-1,1), grpd['response'])\n",
        "print(f'slope: {lin_mean.coef_[0]:.4f}, intercept: {lin_mean.intercept_:.4f}')"
      ],
      "metadata": {
        "id": "umanPihVoAD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "pENhRt1RuWnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slight problem with the simple linear model fitted above is that it's just *obviously rubbish*. The response variable can't be less than 0 or greater than 1 — either you saw the spot or you didn't.\n",
        "\n",
        "We know from Lecture 1 that for these kind of threshold detection tests we expect the **psychometric function** to take a roughly S-shaped or **sigmoidal** form. A variety of such functions are used, but one very popular choice is the **logistic** or **sigmoid function**:\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
        "Fitting this function to data is another very common machine learning model, often used for [binary classification](https://en.wikipedia.org/wiki/Binary_classification).\n",
        "\n",
        "Once again, Scikit-Learn has us covered, with the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model. We can fit this almost exactly as we did the linear regression model above. (Note that LogisticRegression expects the measured values to be 0 or 1, so we'll use the original measurements rather than the grouped means to fit here.)"
      ],
      "metadata": {
        "id": "4-tUV6mJ49oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic = LogisticRegression()\n",
        "logistic.fit(stim['colour'].values.reshape(-1,1), stim['response'])"
      ],
      "metadata": {
        "id": "5qddaFiSo9Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To plot the fit, we'll use the model to make predictions for a bunch of different values and plot those. LogisticRegression supports two kinds of predictions: label predictions (with the method `predict`), which are 0 or 1, and probability predictions (with `predict_proba`) which are continuous in [0, 1]. We want to plot the smooth curve, so we'll use the latter:"
      ],
      "metadata": {
        "id": "G5VELas9_M37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_x = np.linspace(0, 20, 200)\n",
        "pred_y = logistic.predict_proba(pred_x.reshape(-1,1))[:,1]\n",
        "\n",
        "plt.plot(pred_x, pred_y)\n",
        "plt.scatter(grpd.index, grpd['response'])\n",
        "\n",
        "print(f'fitted threshold: {pred_x[pred_y >= 0.5][0]:.4f}')"
      ],
      "metadata": {
        "id": "9y69Xr0vuCue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## psignifit"
      ],
      "metadata": {
        "id": "-tlQRFdDugeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a good and versatile model and its fit to this particular dataset is excellent. Psychophysics data can often be a lot dirtier than this, though, and more sophisticated domain-specific fitting may give better results. The [psignifit](https://github.com/wichmann-lab/python-psignifit/wiki/Basic-Usage) library is designed specifically for fitting psychometric functions to [overdispersed](https://www.sciencedirect.com/science/article/pii/S0042698916000390) data.\n",
        "\n",
        "The library expects the data to be provided in a slightly form, as an array with three columns representing: **stimulus value**, **number of correct trials**, **total number of trials**. We can construct this fairly easily from our grouped data:"
      ],
      "metadata": {
        "id": "SdvH80u-AZFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "levels = grpd.index.values\n",
        "hits = grpd['response'].values * 4\n",
        "totals = np.ones_like(hits) * 4\n",
        "psg_data = np.stack([levels, hits, totals]).T"
      ],
      "metadata": {
        "id": "WQrlRcWnolhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need a couple of options specifying the kind of experiment and what kind of sigmoid to fit."
      ],
      "metadata": {
        "id": "GMVokwwpCDMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psg_options = { 'expType': 'YesNo', 'sigmoidName': 'norm' }"
      ],
      "metadata": {
        "id": "XhBbwV-RsftZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can just let it do its stuff. (Note that psignifit is significantly slower than logistic regression and will throw up a few warnings with this dataset. Just bear with it.)"
      ],
      "metadata": {
        "id": "AwnLb5RgCKmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "psg_fit = ps.psignifit(psg_data, psg_options)"
      ],
      "metadata": {
        "id": "-s_fMOp6s1qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extract the fitted threshold from the `Fit` field of the resulting object:"
      ],
      "metadata": {
        "id": "4Seyzk7PChWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'threshold: {psg_fit[\"Fit\"][0]:.4f}')"
      ],
      "metadata": {
        "id": "NTBo-jBXtL_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's marginally different from the one we got from logistic regression, although in this case they're very close: the data is not ambiguous.\n",
        "\n",
        "We can also generate a plot of the fit. Again, it's very similar to logistic regression in this case."
      ],
      "metadata": {
        "id": "SUWbwFqaDAsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps.psigniplot.plotPsych(psg_fit)"
      ],
      "metadata": {
        "id": "vu4XBE37tnLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Over to you...\n",
        "\n",
        "At this point you should have a sense of a few options for basic analysis of psychophysics data. Now it's your turn to try them out on your own data, or tweak them to investigate other aspects.\n",
        "\n",
        "As always, feel free to ask the tutors if you have any queries."
      ],
      "metadata": {
        "id": "pW6wgUNbDbMH"
      }
    }
  ]
}